{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ceb9d5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from IPython.display import display, Video\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0a855de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\kgane/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2023-9-13 Python-3.9.17 torch-2.0.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "# Load your YOLOv5 model\n",
    "model = torch.hub.load('ultralytics/yolov5:master', 'yolov5s')  # You need to replace this with your model loading code\n",
    "\n",
    "def detect_person_yolov5_video(input_video_path, output_video_path, confidence_threshold=0.5):\n",
    "    # Open the video file for reading\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    \n",
    "    # Get the total number of frames in the video\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Get the video's frame dimensions and frame rate\n",
    "    frame_width = int(cap.get(3))\n",
    "    frame_height = int(cap.get(4))\n",
    "    frame_rate = int(cap.get(5))\n",
    "\n",
    "    # Create VideoWriter object to save the annotated video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, frame_rate, (frame_width, frame_height))\n",
    "    frame_number = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()  # Read a frame from the video\n",
    "\n",
    "        if not ret:\n",
    "            break  # Break the loop when we reach the end of the video\n",
    "\n",
    "        # Perform inference on the frame\n",
    "        results = model(frame)\n",
    "\n",
    "        # Get detected objects with 'person' class and their bounding boxes, class labels, and confidence scores\n",
    "        detected_people = results.pred[0][results.pred[0][:, 5] == 0]  # Class index 0 corresponds to 'person'\n",
    "        bounding_boxes = detected_people[:, :4].cpu().numpy()  # Extract bounding boxes\n",
    "        confidences = detected_people[:, 4].cpu().numpy()  # Extract confidence scores\n",
    "\n",
    "        # Get class names\n",
    "        class_names = results.names[0]\n",
    "\n",
    "        # Draw bounding boxes on the frame\n",
    "        for i in range(len(bounding_boxes)):\n",
    "            bbox = bounding_boxes[i]\n",
    "            confidence = confidences[i]\n",
    "            x_min, y_min, x_max, y_max = map(int, bbox)\n",
    "            label = f\"{class_names}: {confidence:.2f}\"\n",
    "            color = (0, 0, 255)  # Red color for bounding box\n",
    "            thickness = 2\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            font_scale = 0.5\n",
    "            font_color = (0, 0, 255)  # Red color for text\n",
    "            line_type = cv2.LINE_AA\n",
    "\n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), color, thickness)\n",
    "\n",
    "            # Add label (class name and confidence score) as text\n",
    "            cv2.putText(frame, label, (x_min + 5, y_min + 15), font, font_scale, font_color, thickness, line_type)\n",
    "\n",
    "        # Write the annotated frame to the output video\n",
    "        out.write(frame)\n",
    "        \n",
    "        frame_number += 1\n",
    "        progress_percentage = (frame_number / total_frames) * 100\n",
    "        print(f\"Processing Frame {frame_number}/{total_frames} ({progress_percentage:.2f}%)\", end='\\r')\n",
    "\n",
    "    \n",
    "    # Release video objects\n",
    "    cap.release()\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aea854c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Frame 15007/15007 (100.00%)\r"
     ]
    }
   ],
   "source": [
    "input_video_path = 'Videos/Changing an Absorbent Brief for a Bed bound Patient.mp4'\n",
    "output_video_path = 'Videos/output_Changing an Absorbent Brief for a Bed bound Patient.avi'\n",
    "detect_person_yolov5_video(input_video_path, output_video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05cd356e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Frame 691/691 (100.00%)\r"
     ]
    }
   ],
   "source": [
    "input_video_path = 'Videos/NHC - TERi™ Geriatric Care Trainer.mp4'\n",
    "output_video_path = 'Videos/output_NHC - TERi™ Geriatric Care Trainer.avi'\n",
    "detect_person_yolov5_video(input_video_path, output_video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "916b28bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Frame 14311/14311 (100.00%)\r"
     ]
    }
   ],
   "source": [
    "input_video_path = 'Videos/A215.avi'\n",
    "output_video_path = 'Videos/Output_A215.avi'\n",
    "detect_person_yolov5_video(input_video_path, output_video_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
